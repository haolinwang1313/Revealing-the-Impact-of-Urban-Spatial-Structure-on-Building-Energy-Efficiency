                      
"""
Compute key urban form indicators for Xinwu District, Wuxi.

Outputs
-------
1. 500 m grid (GeoPackage + CSV) enriched with:
       - Building-derived metrics (counts, footprint, CI, VCI, etc.)
       - Land-use area breakdown (Residential / Commercial / Office / Education / Industrial)
       - Land-use mix indices (LUM, adjacency, intensity, proximity)
2. 50 m sub-grid with dominant land-use category per cell (GeoPackage layer).

The script relies exclusively on datasets under ``Data/Processed`` that were
generated by ``scripts/preprocess_xinwu.py``.
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Tuple

import geopandas as gpd
import numpy as np
import pandas as pd
from shapely.geometry import box, Point, LineString
import momepy
import networkx as nx

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "Data"
PROCESSED_DIR = DATA_DIR / "Processed"
BOUNDARY_PATH = PROCESSED_DIR / "Boundary" / "xinwu_boundary_32650.geojson"
BUILDING_PATH = PROCESSED_DIR / "Building" / "xinwu_buildings.gpkg"
LAND_USE_PATH = PROCESSED_DIR / "Land_Use" / "xinwu_land_use.gpkg"
ROAD_PATH = PROCESSED_DIR / "Road" / "xinwu_road_centerlines.gpkg"
ROAD_INTEGRATION_PATH = PROCESSED_DIR / "Road" / "xinwu_road_integration.gpkg"

GRID_DIR = PROCESSED_DIR / "Grids"
INDICATOR_DIR = PROCESSED_DIR / "Indicators"

MAIN_CELL_SIZE = 250.0          
SUB_CELL_SIZE = 25.0

if MAIN_CELL_SIZE % SUB_CELL_SIZE != 0:
    raise ValueError("Main grid size must be divisible by sub-grid size.")

MAIN_SIZE_INT = int(MAIN_CELL_SIZE)
SUB_SIZE_INT = int(SUB_CELL_SIZE)

GRID_MAIN_LAYER = f"grid_{MAIN_SIZE_INT}m"
GRID_SUB_LAYER = f"grid_{SUB_SIZE_INT}m"
GRID_SUB_USE_LAYER = f"{GRID_SUB_LAYER}_landuse"
OUTPUT_GRID_LAYER = f"urban_form_{MAIN_SIZE_INT}m"

GRID_MAIN_PATH = GRID_DIR / f"xinwu_{GRID_MAIN_LAYER}.gpkg"
GRID_SUB_PATH = GRID_DIR / f"xinwu_{GRID_SUB_LAYER}.gpkg"
OUTPUT_GRID_PATH = GRID_DIR / f"xinwu_{OUTPUT_GRID_LAYER}.gpkg"
OUTPUT_CSV_PATH = INDICATOR_DIR / f"xinwu_urban_form_{MAIN_SIZE_INT}m.csv"

EULUC_TO_USE: Dict[int, str] = {
    0: "residential",
    1: "office",
    2: "commercial",
    3: "industrial",
    7: "education",
}

USE_CATEGORIES = ["residential", "commercial", "office", "education", "industrial"]
AREA_COLUMNS = {cat: f"area_{cat}_m2" for cat in USE_CATEGORIES}
OTHER_AREA_COLUMN = "area_other_m2"
OTHER_LABEL = "other"

ADJ_DELTAS = [(1, 0), (0, 1)]                                


@dataclass
class GridSpec:
    cell_size: float
    id_field: str
    layer: str


GRID_SPECS: Dict[str, GridSpec] = {
    "main": GridSpec(MAIN_CELL_SIZE, "grid_id_main", GRID_MAIN_LAYER),
    "sub": GridSpec(SUB_CELL_SIZE, "grid_id_sub", GRID_SUB_LAYER),
}


def ensure_directories() -> None:
    GRID_DIR.mkdir(parents=True, exist_ok=True)
    INDICATOR_DIR.mkdir(parents=True, exist_ok=True)


def load_inputs() -> Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:
    if not BOUNDARY_PATH.exists():
        raise FileNotFoundError(f"Boundary file missing: {BOUNDARY_PATH}")
    if not BUILDING_PATH.exists():
        raise FileNotFoundError(f"Building file missing: {BUILDING_PATH}")
    if not LAND_USE_PATH.exists():
        raise FileNotFoundError(f"Land-use file missing: {LAND_USE_PATH}")

    boundary = gpd.read_file(BOUNDARY_PATH)
    buildings = gpd.read_file(BUILDING_PATH)
    land_use = gpd.read_file(LAND_USE_PATH)

    if buildings.crs != boundary.crs:
        buildings = buildings.to_crs(boundary.crs)
    if land_use.crs != boundary.crs:
        land_use = land_use.to_crs(boundary.crs)
    return boundary, buildings, land_use


def compute_base_bounds(boundary: gpd.GeoDataFrame) -> Tuple[float, float, float, float]:
    """Compute snap-aligned bounds shared by all grids."""
    minx, miny, maxx, maxy = boundary.total_bounds
    base_size = max(spec.cell_size for spec in GRID_SPECS.values())
    start_x = math.floor(minx / base_size) * base_size
    start_y = math.floor(miny / base_size) * base_size
    end_x = math.ceil(maxx / base_size) * base_size
    end_y = math.ceil(maxy / base_size) * base_size
    return start_x, start_y, end_x, end_y


def generate_grid(
    boundary: gpd.GeoDataFrame, spec: GridSpec, base_bounds: Tuple[float, float, float, float]
) -> gpd.GeoDataFrame:
    size = spec.cell_size
    base_minx, base_miny, base_maxx, base_maxy = base_bounds
    start_x = math.floor(base_minx / size) * size
    start_y = math.floor(base_miny / size) * size
    end_x = math.ceil(base_maxx / size) * size
    end_y = math.ceil(base_maxy / size) * size

    x_vals = np.arange(start_x, end_x, size)
    y_vals = np.arange(start_y, end_y, size)

    geometries = []
    rows = []
    cols = []
    for row_idx, y in enumerate(y_vals):
        for col_idx, x in enumerate(x_vals):
            geometries.append(box(x, y, x + size, y + size))
            rows.append(row_idx)
            cols.append(col_idx)

    grid = gpd.GeoDataFrame(
        {"geometry": geometries, "row_raw": rows, "col_raw": cols}, crs=boundary.crs
    )
    grid = gpd.overlay(grid, boundary[["geometry"]], how="intersection", keep_geom_type=False)
    grid[spec.id_field] = np.arange(len(grid), dtype=int)
    grid["cell_area_m2"] = grid.geometry.area
    return grid


def assign_parent_ids(
    grid_sub: gpd.GeoDataFrame,
    grid_main: gpd.GeoDataFrame,
    ratio: int,
    main_spec: GridSpec,
) -> gpd.GeoDataFrame:
    lookup = {
        (row, col): gid
        for row, col, gid in zip(
            grid_main["row_raw"], grid_main["col_raw"], grid_main[main_spec.id_field]
        )
    }
    parent_rows = (grid_sub["row_raw"] // ratio).astype(int)
    parent_cols = (grid_sub["col_raw"] // ratio).astype(int)
    parent_ids = []
    for row, col in zip(parent_rows, parent_cols):
        gid = lookup.get((row, col))
        if gid is None:
            raise ValueError("Unable to locate parent 500 m grid for a 50 m cell.")
        parent_ids.append(gid)
    grid_sub = grid_sub.copy()
    grid_sub[main_spec.id_field] = parent_ids
    return grid_sub


def attach_buildings_to_grid(
    buildings: gpd.GeoDataFrame, grid_main: gpd.GeoDataFrame, main_spec: GridSpec
) -> gpd.GeoDataFrame:
    points = buildings.copy()
    points["geometry"] = points.geometry.centroid
    joined = gpd.sjoin(
        points, grid_main[[main_spec.id_field, "geometry"]], how="left", predicate="within"
    )
    joined = joined.rename(columns={main_spec.id_field: "grid_id"})
    joined = joined.drop(columns=["index_right"])
    return joined


def _pairwise_average(values: np.ndarray, denom: np.ndarray) -> float:
    with np.errstate(divide="ignore", invalid="ignore"):
        matrix = values / denom
    mask = np.triu(np.ones(matrix.shape, dtype=bool), k=1)
    data = matrix[mask]
    data = data[np.isfinite(data)]
    if data.size == 0:
        return float("nan")
    return float(data.mean())


def compute_ci_vci(buildings_pts: gpd.GeoDataFrame) -> pd.DataFrame:
    results = []
    grouped = buildings_pts.dropna(subset=["grid_id"]).groupby("grid_id")
    for grid_id, group in grouped:
        n = len(group)
        if n < 2:
            results.append({"grid_id": grid_id, "ci": math.nan, "vci": math.nan})
            continue
        coords = np.column_stack((group.geometry.x.values, group.geometry.y.values))
        dx = (coords[:, 0][:, None] - coords[:, 0][None, :]) / 1000.0                 
        dy = (coords[:, 1][:, None] - coords[:, 1][None, :]) / 1000.0
        dist2 = dx**2 + dy**2
        np.fill_diagonal(dist2, np.nan)
        areas = group["footprint_area_m2"].values / 10_000.0                    
        heights = group["height_m"].values
        if np.isnan(heights).all():
            heights = np.zeros_like(areas)
        else:
            mean_height = np.nanmean(heights)
            heights = np.nan_to_num(heights, nan=float(mean_height))
        area_products = np.outer(areas, areas)
        ci_value = _pairwise_average(area_products, dist2)
        volumes = areas * heights
        volume_products = np.outer(volumes, volumes)
        dz = ((heights / 2.0)[:, None] - (heights / 2.0)[None, :]) / 1000.0                 
        dist3 = np.sqrt(dist2 + dz**2)
        np.fill_diagonal(dist3, np.nan)
        vci_value = _pairwise_average(volume_products, dist3)
        results.append({"grid_id": grid_id, "ci": ci_value, "vci": vci_value})
    return pd.DataFrame(results)


def aggregate_building_stats(buildings_pts: gpd.GeoDataFrame) -> pd.DataFrame:
    agg = (
        buildings_pts.dropna(subset=["grid_id"])
        .groupby("grid_id")
        .agg(
            building_count=("grid_id", "size"),
            total_footprint_m2=("footprint_area_m2", "sum"),
            mean_height_m=("height_m", "mean"),
        )
    )
    return agg.reset_index()


def classify_subgrid_land_use(
    grid_sub: gpd.GeoDataFrame, land_use: gpd.GeoDataFrame, sub_spec: GridSpec, main_spec: GridSpec
) -> Tuple[gpd.GeoDataFrame, pd.DataFrame]:
    keep_cols = [
        sub_spec.id_field,
        main_spec.id_field,
        "cell_area_m2",
        "row_raw",
        "col_raw",
        "geometry",
    ]
    overlay = gpd.overlay(grid_sub[keep_cols], land_use[["Class", "geometry"]], how="intersection")
    if overlay.empty:
        raise RuntimeError("Land-use overlay returned no records; please verify inputs.")
    overlay["area_m2"] = overlay.geometry.area
    overlay["use_cat"] = overlay["Class"].map(EULUC_TO_USE)
    overlay = overlay.dropna(subset=["use_cat"])
    area = (
        overlay.groupby([sub_spec.id_field, main_spec.id_field, "use_cat"])["area_m2"]
        .sum()
        .reset_index()
    )

    cell_area = grid_sub.set_index(sub_spec.id_field)["cell_area_m2"]
    covered = area.groupby(sub_spec.id_field)["area_m2"].sum().reindex(cell_area.index).fillna(0.0)
    residual = (cell_area - covered).clip(lower=0)
    residual_df = (
        residual[residual > 0]
        .rename("area_m2")
        .reset_index()
        .merge(grid_sub[[sub_spec.id_field, main_spec.id_field]], on=sub_spec.id_field)
    )
    if not residual_df.empty:
        residual_df["use_cat"] = OTHER_LABEL
        area = pd.concat([area, residual_df], ignore_index=True)

    area = area.rename(columns={"area_m2": "lu_area_m2"})
    area["cell_area_m2"] = area[sub_spec.id_field].map(cell_area)
    area["share"] = area["lu_area_m2"] / area["cell_area_m2"]

    idx = area.groupby(sub_spec.id_field)["lu_area_m2"].idxmax()
    dominant = (
        area.loc[idx, [sub_spec.id_field, "use_cat", "share"]]
        .rename(columns={"use_cat": "use_category", "share": "dominance_share"})
    )

    subgrid = grid_sub.merge(dominant, on=sub_spec.id_field, how="left")
    subgrid["use_category"] = subgrid["use_category"].fillna(OTHER_LABEL)
    subgrid["dominance_share"] = subgrid["dominance_share"].fillna(0.0)
    subgrid["centroid_x"] = subgrid.geometry.centroid.x
    subgrid["centroid_y"] = subgrid.geometry.centroid.y
    return subgrid, area


def compute_area_breakdown(area: pd.DataFrame, main_spec: GridSpec) -> pd.DataFrame:
    pivot = (
        area.groupby([main_spec.id_field, "use_cat"])["lu_area_m2"]
        .sum()
        .unstack(fill_value=0)
    )
    for cat in [*USE_CATEGORIES, OTHER_LABEL]:
        if cat not in pivot.columns:
            pivot[cat] = 0.0
    ordered = pivot[[*USE_CATEGORIES, OTHER_LABEL]].copy()
    for cat in USE_CATEGORIES:
        ordered.rename(columns={cat: AREA_COLUMNS[cat]}, inplace=True)
    ordered.rename(columns={OTHER_LABEL: OTHER_AREA_COLUMN}, inplace=True)
    ordered["total_area_m2"] = ordered.sum(axis=1)
    ordered = ordered.reset_index()
    return ordered


def shannon_entropy(row: pd.Series) -> float:
    values = row[row > 0]
    if values.empty:
        return math.nan
    probs = values / values.sum()
    entropy = -(probs * np.log(probs)).sum()
    return float(entropy / math.log(len(USE_CATEGORIES)))


def adjacency_score(group: pd.DataFrame) -> float:
    if len(group) < 2:
        return math.nan
    lookup = {(r, c): use for r, c, use in zip(group["row_raw"], group["col_raw"], group["use_category"])}
    total_edges = 0
    same_edges = 0
    for (row, col), use in lookup.items():
        for dr, dc in ADJ_DELTAS:
            neighbor = (row + dr, col + dc)
            neighbor_use = lookup.get(neighbor)
            if neighbor_use is None:
                continue
            total_edges += 1
            if neighbor_use == use:
                same_edges += 1
    if total_edges == 0:
        return math.nan
    return same_edges / total_edges


def proximity_score(group: pd.DataFrame) -> float:
    subset = group[group["use_category"] != OTHER_LABEL]
    if subset.empty:
        return math.nan
    centroids = np.column_stack((subset["centroid_x"].values, subset["centroid_y"].values))
    uses = subset["use_category"].values
    scores = []
    for idx in range(len(subset)):
        mask = uses != uses[idx]
        if not mask.any():
            continue
        distances = np.sqrt(((centroids[mask] - centroids[idx]) ** 2).sum(axis=1))
        if distances.size == 0:
            continue
        dmin = distances.min()
        scores.append(1.0 / (1.0 + dmin))
    if not scores:
        return math.nan
    return float(np.mean(scores))


def compute_land_use_metrics(
    subgrid: gpd.GeoDataFrame, area_breakdown: pd.DataFrame, main_spec: GridSpec
) -> pd.DataFrame:
    area_cols = [AREA_COLUMNS[cat] for cat in USE_CATEGORIES]
    entropy = area_breakdown.apply(
        lambda row: shannon_entropy(row[area_cols]), axis=1
    ).rename("lum")
    proportions = area_breakdown[area_cols].div(area_breakdown["total_area_m2"], axis=0)
    proportions = proportions.replace([np.inf, -np.inf], np.nan)
    intensity = proportions.max(axis=1).rename("lum_intensity")

    adjacency_results = []
    proximity_results = []
    for grid_id, group in subgrid.groupby(main_spec.id_field):
        adjacency_results.append({main_spec.id_field: grid_id, "lum_adjacency": adjacency_score(group)})
        proximity_results.append({main_spec.id_field: grid_id, "lum_proximity": proximity_score(group)})
    adjacency_df = pd.DataFrame(adjacency_results)
    proximity_df = pd.DataFrame(proximity_results)

    metrics = area_breakdown[
        [main_spec.id_field, *area_cols, OTHER_AREA_COLUMN, "total_area_m2"]
    ].copy()
    metrics["lum"] = entropy.values
    metrics["lum_intensity"] = intensity.values
    metrics = metrics.merge(adjacency_df, on=main_spec.id_field, how="left")
    metrics = metrics.merge(proximity_df, on=main_spec.id_field, how="left")
    return metrics


def normalise_series(
    series: pd.Series,
    lower_quantile: float | None = None,
    upper_quantile: float | None = None,
) -> pd.Series:
    if lower_quantile is not None:
        low = series.quantile(lower_quantile)
    else:
        low = None
    if upper_quantile is not None:
        high = series.quantile(upper_quantile)
    else:
        high = None

    clipped = series.copy()
    if low is not None:
        clipped = clipped.clip(lower=low)
    if high is not None:
        clipped = clipped.clip(upper=high)

    valid = clipped.dropna()
    if valid.empty:
        return pd.Series(np.nan, index=series.index)
    min_val = valid.min()
    max_val = valid.max()
    if math.isclose(max_val, min_val):
        return pd.Series(np.nan, index=series.index)
    normalized = (clipped - min_val) / (max_val - min_val)
    return normalized


def build_road_graph() -> Tuple[object, gpd.GeoDataFrame]:
    if not ROAD_PATH.exists():
        raise FileNotFoundError(f"Road dataset missing: {ROAD_PATH}")
    roads = gpd.read_file(ROAD_PATH, layer="road_centerlines")
    roads = roads[roads.geometry.is_valid & (~roads.geometry.is_empty)].copy()
    if "length_m" not in roads.columns:
        roads["length_m"] = roads.geometry.length
    G = momepy.gdf_to_nx(roads, approach="primal", length="length_m")
    return G, roads


def closeness_with_radius(
    G: nx.Graph, radius: float, weight: str = "length_m"
) -> Dict[object, float]:
    results: Dict[object, float] = {}
    for node in G.nodes:
        lengths = nx.single_source_dijkstra_path_length(
            G, node, cutoff=radius, weight=weight
        )
        if len(lengths) <= 1:
            results[node] = 0.0
            continue
        total = sum(lengths.values())
        results[node] = (len(lengths) - 1) / total if total > 0 else 0.0
    return results


def compute_integration_metrics(
    grid_main: gpd.GeoDataFrame, main_spec: GridSpec
) -> pd.DataFrame:
    G, _ = build_road_graph()
    global_cc = nx.closeness_centrality(G, distance="length_m", wf_improved=False)
    local_cc = closeness_with_radius(G, radius=500, weight="length_m")

    node_records = []
    for node_id in G.nodes:
        node_records.append(
            {
                "geometry": Point(node_id),
                "global_integration": global_cc.get(node_id, 0.0),
                "local_integration": local_cc.get(node_id, 0.0),
            }
        )
    nodes = gpd.GeoDataFrame(node_records, geometry="geometry", crs=grid_main.crs)

    edge_records = []
    for u, v, data in G.edges(data=True):
        geom = data.get("geometry")
        if geom is None:
            geom = LineString([Point(u), Point(v)])
        gi_val = (global_cc.get(u, 0.0) + global_cc.get(v, 0.0)) / 2
        li_val = (local_cc.get(u, 0.0) + local_cc.get(v, 0.0)) / 2
        edge_records.append(
            {
                "geometry": geom,
                "gi_line": gi_val,
                "li_line": li_val,
            }
        )
    edges = gpd.GeoDataFrame(edge_records, geometry="geometry", crs=grid_main.crs)
    edges["gi_norm_line"] = normalise_series(
        edges["gi_line"], lower_quantile=0.05, upper_quantile=0.95
    )
    edges["li_norm_line"] = normalise_series(
        edges["li_line"], lower_quantile=0.05, upper_quantile=0.95
    )
    if ROAD_INTEGRATION_PATH.exists():
        ROAD_INTEGRATION_PATH.unlink()
    edges.to_file(ROAD_INTEGRATION_PATH, layer="integration_lines", driver="GPKG")

    joined = gpd.sjoin(
        nodes, grid_main[[main_spec.id_field, "geometry"]], how="left", predicate="within"
    )
    grouped = (
        joined.dropna(subset=[main_spec.id_field])
        .groupby(main_spec.id_field)
        .agg(
            gi_mean=("global_integration", "mean"),
            li_mean=("local_integration", "mean"),
        )
        .reset_index()
    )
    grouped["gi_norm"] = normalise_series(
        grouped["gi_mean"], lower_quantile=0.05, upper_quantile=0.95
    )
    grouped["li_norm"] = normalise_series(
        grouped["li_mean"], lower_quantile=0.05, upper_quantile=0.95
    )
    return grouped


def main() -> None:
    ensure_directories()
    boundary, buildings, land_use = load_inputs()

    base_bounds = compute_base_bounds(boundary)
    grids: Dict[str, gpd.GeoDataFrame] = {}
    for key, spec in GRID_SPECS.items():
        grid = generate_grid(boundary, spec, base_bounds)
        grids[key] = grid

    ratio = int(GRID_SPECS["main"].cell_size / GRID_SPECS["sub"].cell_size)

    grid_main = grids["main"]
    grid_sub = assign_parent_ids(grids["sub"], grid_main, ratio, GRID_SPECS["main"])
    cell_area_series = grid_main.set_index(GRID_SPECS["main"].id_field)["cell_area_m2"]

    building_points = attach_buildings_to_grid(buildings, grid_main, GRID_SPECS["main"])
    building_summary = aggregate_building_stats(building_points).rename(columns={"grid_id": GRID_SPECS["main"].id_field})
    ci_vci = compute_ci_vci(building_points).rename(columns={"grid_id": GRID_SPECS["main"].id_field})

    building_metrics = building_summary.merge(ci_vci, on=GRID_SPECS["main"].id_field, how="left")
    building_metrics["ci_log"] = np.log1p(building_metrics["ci"])
    building_metrics["vci_log"] = np.log1p(building_metrics["vci"])
    building_metrics["ci_norm"] = normalise_series(
        building_metrics["ci_log"], lower_quantile=0.05, upper_quantile=0.95
    )
    building_metrics["vci_norm"] = normalise_series(
        building_metrics["vci_log"], lower_quantile=0.05, upper_quantile=0.95
    )
    building_metrics["building_density"] = (
        building_metrics["total_footprint_m2"]
        / building_metrics[GRID_SPECS["main"].id_field].map(cell_area_series)
    )

    subgrid_use, area_table = classify_subgrid_land_use(
        grid_sub, land_use, GRID_SPECS["sub"], GRID_SPECS["main"]
    )
    area_breakdown = compute_area_breakdown(area_table, GRID_SPECS["main"])
    land_use_metrics = compute_land_use_metrics(subgrid_use, area_breakdown, GRID_SPECS["main"])
    land_use_metrics["lum_norm"] = normalise_series(land_use_metrics["lum"])
    land_use_metrics["lum_adjacency_norm"] = normalise_series(land_use_metrics["lum_adjacency"])
    land_use_metrics["lum_intensity_norm"] = normalise_series(land_use_metrics["lum_intensity"])
    land_use_metrics["lum_proximity_norm"] = normalise_series(land_use_metrics["lum_proximity"])

    indicators = grid_main[[GRID_SPECS["main"].id_field]].merge(
        building_metrics, on=GRID_SPECS["main"].id_field, how="left"
    )
    indicators = indicators.merge(land_use_metrics, on=GRID_SPECS["main"].id_field, how="left")
    integration_metrics = compute_integration_metrics(grid_main, GRID_SPECS["main"])
    indicators = indicators.merge(integration_metrics, on=GRID_SPECS["main"].id_field, how="left")

    grid_stats = grid_main.merge(indicators, on=GRID_SPECS["main"].id_field, how="left")
    fill_zero_cols = ["building_count", "total_footprint_m2", "building_density"]
    for col in fill_zero_cols:
        if col in grid_stats.columns:
            grid_stats[col] = grid_stats[col].fillna(0.0)
    for col in ["gi_mean", "li_mean", "gi_norm", "li_norm"]:
        if col in grid_stats.columns:
            grid_stats[col] = grid_stats[col].fillna(0.0)
    grid_stats.to_file(OUTPUT_GRID_PATH, layer=OUTPUT_GRID_LAYER, driver="GPKG")
    grid_stats.drop(columns="geometry").to_csv(OUTPUT_CSV_PATH, index=False)

    grid_main.to_file(GRID_MAIN_PATH, layer=GRID_MAIN_LAYER, driver="GPKG")
    grid_sub.to_file(GRID_SUB_PATH, layer=GRID_SUB_LAYER, driver="GPKG")
    subgrid_use.to_file(GRID_SUB_PATH, layer=GRID_SUB_USE_LAYER, driver="GPKG", mode="a")

    print(f"Saved main grid to {GRID_MAIN_PATH}")
    print(f"Saved sub grid to {GRID_SUB_PATH}")
    print(f"Urban form indicators written to {OUTPUT_GRID_PATH}")
    print(f"Indicator table exported to {OUTPUT_CSV_PATH}")


if __name__ == "__main__":
    main()
